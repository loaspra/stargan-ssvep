{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FBCCA and Extended FBCCA for SSVEP signals classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "%pip install ffmpeg\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "import scipy.io as sio\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from scipy.signal import butter, lfilter, periodogram\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('expr/checkpoints/SSVEP_w_2s_log.txt')\n",
    "\"\"\"\n",
    "Index(['time', 'iteration', 'D/latent_real', 'D/latent_fake', 'D/latent_reg',\n",
    "       'D/ref_real', 'D/ref_fake', 'D/ref_reg', 'G/latent_adv', 'G/latent_sty',\n",
    "       'G/latent_ds', 'G/latent_cyc', 'G/ref_adv', 'G/ref_sty', 'G/ref_ds',\n",
    "       'G/ref_cyc', 'G/lambda_ds'],\n",
    "      dtype='object')\n",
    "\"\"\"\n",
    "print(df.head())\n",
    "# Plots: \n",
    "# Disceriminators:\n",
    "# plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['iteration'], df['G/ref_sty'], label='Latent')\n",
    "plt.plot(df['iteration'], df['G/latent_sty'], label='Reference')\n",
    "plt.title('Style Reconstruction Loss')\n",
    "plt.legend()\n",
    "plt.ylim(0, 3)\n",
    "plt.show()\n",
    "\n",
    "# # Generators:\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# # plt.plot(df['iteration'], df['G/lambda_ds'], label='G/lambda_ds')\n",
    "# plt.plot(df['iteration'], df['D/ref_fake'], label='G/ref_fake')\n",
    "# plt.plot(df['iteration'], df['D/ref_real'], label='G/latent_real')\n",
    "# # plt.plot(df['iteration'], df['G/ref_ds'], label='G/ref_ds')\n",
    "# plt.ylim(0, 3)\n",
    "# plt.title('Generator Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the src data for the model: \n",
    "os.makedirs('data/colorado/processed', exist_ok=True)\n",
    "\n",
    "subjects = ['s1_trial1.csv', 's2_trial2.csv', 's3_trial3.csv', 's4_trial4.csv']\n",
    "for i, subject in enumerate(subjects):\n",
    "\tdf = pd.read_csv(f'data/colorado/processed/{subject}')\n",
    "\n",
    "\tos.makedirs(f'data/colorado/test/{i}', exist_ok=True)\n",
    "\n",
    "\tsegment = df.iloc[:1024].values.swapaxes(0, 1)\n",
    "\tnp.save(f'data/colorado/test/{i}/1024_segment.npy', segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "\n",
    "\n",
    "class Args:\n",
    "    img_size = 1024\n",
    "    num_domains = 4\n",
    "    latent_dim = 16\n",
    "    hidden_dim = 512\n",
    "    style_dim = 64\n",
    "    lambda_reg = 1\n",
    "    lambda_cyc = 1\n",
    "    lambda_sty = 1\n",
    "    lambda_ds = 1\n",
    "    ds_iter = 5000\n",
    "    w_hpf = 0  # For SSVEP\n",
    "    randcrop_prob = 0.5\n",
    "    total_iters = 5000\n",
    "    resume_iter = 0\n",
    "    batch_size = 8\n",
    "    val_batch_size = 8\n",
    "    lr = 5e-4\n",
    "    f_lr = 1e-6\n",
    "    beta1 = 0.0\n",
    "    beta2 = 0.99\n",
    "    weight_decay = 1e-4\n",
    "    num_outs_per_domain = 2\n",
    "    mode = 'train'\n",
    "    num_workers = 16\n",
    "    seed = 777\n",
    "    train_img_dir = 'data/train'\n",
    "    val_img_dir = 'data/val'\n",
    "    # sample_dir = 'expr/samples'\n",
    "    checkpoint_dir = 'expr/checkpoints'\n",
    "    eval_dir = 'expr/eval'\n",
    "    result_dir = 'expr/results'\n",
    "    print_every = 10\n",
    "    sample_every = 250\n",
    "    save_every = 1000\n",
    "    eval_every = 2500\n",
    "    runType = f'w_4s'\n",
    "    sample_dir = f'expr/samples/{runType}'\n",
    "\n",
    "args = Args()\n",
    "args.skip_data = False  # Add this line to initialize skip_data attribute\n",
    "\n",
    "makedirs(args.sample_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Number of parameters of generator: 12161329\n",
      "Number of parameters of mapping_network: 4079872\n",
      "Number of parameters of style_encoder: 7008622\n",
      "Number of parameters of discriminator: 6879346\n",
      "Initializing generator...\n",
      "Initializing mapping_network...\n",
      "Initializing style_encoder...\n",
      "Initializing discriminator...\n",
      "Preparing DataLoader for the generation phase...\n",
      "Preparing DataLoader for the generation phase...\n",
      "Loading checkpoint from expr/checkpoints/SSVEP_w_4s_nets.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loaspra/stargan-ssvep/core/checkpoint.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  module_dict = torch.load(fname)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from expr/checkpoints/SSVEP_w_4s_nets_ema.ckpt...\n",
      "Loading checkpoint from expr/checkpoints/SSVEP_w_4s_optims.ckpt...\n",
      "Working on latent-guided image synthesis...\n",
      "Translating using latent with x_src shape: torch.Size([4, 3, 1024])\n",
      "Saving latent into expr/samples/w_4s/latent_psi_0.5.jpg_latent_0.png\n",
      "Saving latent into expr/samples/w_4s/latent_psi_0.5.jpg_latent_1.png\n",
      "Saving latent into expr/samples/w_4s/latent_psi_0.5.jpg_latent_2.png\n",
      "Saving latent into expr/samples/w_4s/latent_psi_0.5.jpg_latent_3.png\n",
      "Translating using latent with x_src shape: torch.Size([4, 3, 1024])\n",
      "Saving latent into expr/samples/w_4s/latent_psi_0.7.jpg_latent_0.png\n",
      "Saving latent into expr/samples/w_4s/latent_psi_0.7.jpg_latent_1.png\n",
      "Saving latent into expr/samples/w_4s/latent_psi_0.7.jpg_latent_2.png\n",
      "Saving latent into expr/samples/w_4s/latent_psi_0.7.jpg_latent_3.png\n",
      "Translating using latent with x_src shape: torch.Size([4, 3, 1024])\n",
      "Saving latent into expr/samples/w_4s/latent_psi_1.0.jpg_latent_0.png\n",
      "Saving latent into expr/samples/w_4s/latent_psi_1.0.jpg_latent_1.png\n",
      "Saving latent into expr/samples/w_4s/latent_psi_1.0.jpg_latent_2.png\n",
      "Saving latent into expr/samples/w_4s/latent_psi_1.0.jpg_latent_3.png\n",
      "Working on expr/results/ref...\n",
      "Shape of src.x: torch.Size([4, 3, 1024])\n",
      "Translating using reference with x_src shape: torch.Size([4, 3, 1024])\n",
      "Labels: tensor([0, 1, 2, 3], device='cuda:0')\n",
      "Saving into expr/results/ref.png\n",
      "saving into expr/results/ref_03\n"
     ]
    }
   ],
   "source": [
    "SRC_DIR = \"data/colorado/test/\"\n",
    "REF_DIR = \"data/ref/\"\n",
    "NUM_DOMAINS = 4\n",
    "IMG_SIZE = 1024\n",
    "VAL_BATCH_SIZE = 4\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "import os\n",
    "from munch import Munch\n",
    "from core.data_loader import get_test_loader\n",
    "from core.solver import Solver\n",
    "\n",
    "def subdirs(dname):\n",
    "    return [d for d in os.listdir(dname)\n",
    "            if os.path.isdir(os.path.join(dname, d))]\n",
    "\n",
    "# assert len(subdirs(SRC_DIR)) == NUM_DOMAINS?\n",
    "assert len(subdirs(REF_DIR)) == NUM_DOMAINS\n",
    "\n",
    "solver = Solver(args)\n",
    "\n",
    "loaders = Munch(src=get_test_loader(root=SRC_DIR,\n",
    "                                    img_size=IMG_SIZE,\n",
    "                                    batch_size=VAL_BATCH_SIZE,\n",
    "                                    shuffle=False,\n",
    "                                    num_workers=NUM_WORKERS),\n",
    "                ref=get_test_loader(root=REF_DIR,\n",
    "                                    img_size=IMG_SIZE,\n",
    "                                    batch_size=VAL_BATCH_SIZE,\n",
    "                                    shuffle=False,\n",
    "                                    num_workers=NUM_WORKERS))\n",
    "solver.sample(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils2 import generate_ref_signal, combined_cca\n",
    "\n",
    "# SSVEP target frequencies\n",
    "frequencies = [8, 10, 12, 14]\n",
    "# Number of samples\n",
    "N = 1024\n",
    "# Sampling frequency\n",
    "fs = 250\n",
    "# Number of harmonics\n",
    "n_harmonics = 3\n",
    "# Number of sub-bands\n",
    "n_subbands = 8\n",
    "# Filter bank design\n",
    "filter_bank_design = 'M1'\n",
    "# Reference signals\n",
    "ref_signals = generate_ref_signal(\n",
    "    'data/raw/Freq_Phase.mat', frequencies, N, n_harmonics, fs)\n",
    "\n",
    "# combined_cca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_of_subject_ECCA(subject, path, template_path, target, labels, synt_ratio=0.5):\n",
    "\n",
    "    synth_template = np.zeros((4, 6, 3, 1024))\n",
    "    # leave 1 window to perform testing (load 6 out of 7 windows to create the template)\n",
    "    real_template = np.zeros((4, 6, 3, 1024))\n",
    "\n",
    "    # Get the synthetic data for the subject (results of StarGan)\n",
    "    for label in labels:\n",
    "        synth_template[label] = np.load(f\"{template_path}ref_0{label}_b.npy\")[\n",
    "            :6, :, :]  # (8, 3, 1024) 8 is batch_size\n",
    "        # Get the real data for the subject\n",
    "        for i in range(6):\n",
    "            real_template[label, i, :, :] = np.load(\n",
    "                path + f\"{label}/S{subject}_{i}.npy\")\n",
    "\n",
    "    # get the average of the concatenated array\n",
    "    if synt_ratio == 0:\n",
    "        template = np.mean(real_template, axis=1)\n",
    "    elif synt_ratio == 1:\n",
    "        template = np.mean(synth_template, axis=1)\n",
    "    else:\n",
    "        template = np.mean(\n",
    "            np.concatenate((\n",
    "                real_template[:,:(int((1 - synt_ratio) * synth_template.shape[0]))],\n",
    "                synth_template[:,:int(synt_ratio) *\n",
    "                                   synth_template.shape[0]]\n",
    "            ), axis = 1), axis=1)\n",
    "\n",
    "    in_signal = np.load(path + f\"{target}/S{subject}_6.npy\")\n",
    "    # print(f\"Shape of templates: {real_template.shape}, {synth_template.shape}, {template.shape}\")\n",
    "    target_frequency = combined_cca(\n",
    "        in_signal, template, fs, n_subbands, filter_bank_design, ref_signals, 8)\n",
    "\n",
    "    acc = accuracy_score([target], [target_frequency])\n",
    "\n",
    "    return acc\n",
    "\n",
    "def get_acc_of_subject(subject, path, label, w_size=4):\n",
    "\n",
    "    # get all .npy files of the subject\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for file in files:\n",
    "        if re.match(f\"^{subject}_.\\.npy$\", file) and file.endswith(\".npy\"):\n",
    "            in_signal = np.load(path + file)\n",
    "            target_frequency = fbcca(\n",
    "                in_signal, fs, n_subbands, filter_bank_design, ref_signals, 8)\n",
    "            predictions.append(target_frequency)\n",
    "\n",
    "    targets = np.ones(len(predictions)) * label\n",
    "    # calculate the accuracy\n",
    "    acc = accuracy_score(targets, predictions)\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"expr/results/ref_03_w_4s.npy\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "IDX_FREQS = [0, 2, 4, 6]  # [8, 10, 12, 14]\n",
    "\n",
    "file_path = \"data/train/0/S4_2.npy\"\n",
    "segment = np.load(file_path) # .swapaxes(0, 1)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "accuracies = np.zeros((36, 6))\n",
    "\n",
    "dominant_freqs = []\n",
    "# print(segment.shape)\n",
    "for channel in segment:\n",
    "    freqs, psd = signal.welch(channel, fs=250, nperseg=250)\n",
    "    dominant_freq = np.argmax(psd)\n",
    "    print(dominant_freq)\n",
    "    dominant_freqs.append(dominant_freq)\n",
    "if dominant_freqs.count(8) > 1:\n",
    "    accuracies[3, 2] = 1\n",
    "else:\n",
    "    accuracies[3, 2] = 0\n",
    "\n",
    "print(accuracies)\n",
    "dominant_freqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"S1_0.npy\", \"S4_2.npy\", \"S33_1.npy\", \"S33_2.npy\", \"S33_3.npy\", \"S33_4.npy\", \"S33_5.npy\", \"S33_4.npy\"]\n",
    "files = [file for file in files if accuracies[int(file.split(\"_\")[0][1:]) - 1, int(file.split(\"_\")[1][0:1])] == 1]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = np.load(\"data/train/0/S4_2.npy\")\n",
    "print(data.shape)\n",
    "\n",
    "# plot the data\n",
    "sample = data[0, :] # (3, 1024)\n",
    "# data[3, 1, 2, :] --> 12 Hz\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(data)\n",
    "# plt.show()\n",
    "\n",
    "freqs, psd = signal.welch(sample, fs=250, nperseg=250)\n",
    "# set all negatives to 0\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.stem(freqs, psd)\n",
    "\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Power/Frequency (dB/Hz)')\n",
    "plt.title('Power Spectral Density of EEG Signal')\n",
    "plt.xlim(0, 50)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# plot the psd of the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Windows size = 4 seconds\n",
    "acc1_4 = [0.800, 0.810, 0.813, 0.820, 0.827]\n",
    "std1_4 = [0.01, 0.015, 0.012, 0.013, 0.014]\n",
    "\n",
    "# Accuracies using synthetic data\n",
    "acc2_4 = [0.750, 0.760, 0.763, 0.770, 0.777]\n",
    "std2_4 = [0.02, 0.018, 0.017, 0.019, 0.019]\n",
    "\n",
    "################ Windows size = 2 seconds\n",
    "acc1_2 = [0.68, 0.725, 0.71, 0.712, 0.725]\n",
    "std1_2 = [0.03, 0.02, 0.03, 0.019, 0.023]\n",
    "\n",
    "# Accuracies using synthetic data\n",
    "acc2_2 = [0.583, 0.625, 0.62, 0.66, 0.66]\n",
    "std2_2 = [0.032, 0.02, 0.03, 0.019, 0.019]\n",
    "\n",
    "################ Windows size = 1 seconds\n",
    "acc1_1 = [0.66, 0.66, 0.66, 0.725, 0.62]\n",
    "std1_1 = [0.03, 0.02, 0.03, 0.019, 0.023]\n",
    "\n",
    "# Accuracies using synthetic data\n",
    "acc2_1 = [0.66, 0.5, 0.625, 0.705, 0.5]\n",
    "std2_1 = [0.032, 0.02, 0.03, 0.019, 0.019]\n",
    "\n",
    "# Define the dataframe for all the window sizes\n",
    "df_1 = pd.DataFrame({\n",
    "    'Samples': list(range(1, 6)) * 2,\n",
    "    'Accuracy': acc1_1 + acc2_1,\n",
    "    'Data': ['Real'] * 5 + ['Synthetic'] * 5,\n",
    "    'Std': std1_1 + std2_1,\n",
    "    'Window': ['1s'] * 10\n",
    "})\n",
    "\n",
    "df_2 = pd.DataFrame({\n",
    "    'Samples': list(range(1, 6)) * 2,\n",
    "    'Accuracy': acc1_2 + acc2_2,\n",
    "    'Data': ['Real'] * 5 + ['Synthetic'] * 5,\n",
    "    'Std': std1_2 + std2_2,\n",
    "    'Window': ['2s'] * 10\n",
    "})\n",
    "\n",
    "df_4 = pd.DataFrame({\n",
    "    'Samples': list(range(1, 6)) * 2,\n",
    "    'Accuracy': acc1_4 + acc2_4,\n",
    "    'Data': ['Real'] * 5 + ['Synthetic'] * 5,\n",
    "    'Std': std1_4 + std2_4,\n",
    "    'Window': ['4s'] * 10\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_sig = \"expr/samples/w_4s/latent_psi_1.0.jpg_w_4s.npy\"\n",
    "\n",
    "data = np.load(latent_sig)\n",
    "print(data.shape)\n",
    "\n",
    "sample = data[2, 2, :]\n",
    "# data[2, :, :] --> 8hz\n",
    "# data[3 y 4 y 7, :] --> 10hz\n",
    "# data[5 y 6, 2, :] --> 12hz\n",
    "# data[1, 2, :] --> 14hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_path = \"expr/results/vanilla/\"\n",
    "path = \"data/staged/\"\n",
    "labels = [0, 1, 2, 3]\n",
    "\n",
    "bins = 4\n",
    "accs_ratio = np.zeros(bins + 1)\n",
    "\n",
    "for k in range(bins + 1):\n",
    "\n",
    "    accs = np.zeros(36)\n",
    "\n",
    "    for label in tqdm(labels):\n",
    "\n",
    "        path = f\"data/staged/\"\n",
    "\n",
    "        for i in range(1, 36):\n",
    "            acc = get_acc_of_subject_ECCA(\n",
    "                i, path, template_path, label, labels, synt_ratio=(1/bins) * k)\n",
    "            accs[i - 1] += acc * 1 / len(labels)\n",
    "\n",
    "    accs_ratio[k] = np.mean(accs)\n",
    "\n",
    "print(accs_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0, 1, 2, 3]\n",
    "\n",
    "window_size = 4\n",
    "\n",
    "accs = np.zeros(36)\n",
    "\n",
    "for label in tqdm(labels):\n",
    "\n",
    "    path = f\"data/staged/{label}/\"\n",
    "\n",
    "    for i in range(1, 36):\n",
    "        acc = get_acc_of_subject(f\"S{i}\", path, label, window_size)\n",
    "        accs[i - 1] += acc * 1 / len(labels)\n",
    "\n",
    "\n",
    "print(accs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQS = [8, 10, 12, 14]\n",
    "IDX_FREQS = [0, 2, 4, 6]\n",
    "for i, (freq, idX_fre) in enumerate(zip(FREQS, IDX_FREQS)):\n",
    "    print(i, freq, idX_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
